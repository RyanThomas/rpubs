---
title: "Webscraping in R"
author: "Ryan Thomas"
date: "March 15, 2017"
output: html_document
---

## Learning Outcomes
* Know the general process ("steps") for scraping a web page.
* Know how to tell if a page is scrapable before you write the scraper.
* Make a difficulty/benefit estimation of a page's scrapability.
* Know when you should scrape a web page.
+ Option of 'last resort', but it's almost always an option.
* Know where to look for more resources.

# Webscraping in R
This workshop will introduce you to the concept and practices of web scraping in R using the rvest package. In the end, you will have worked through the process of writing a scraper for two websites, used the basic functions of rvest. The process of scraping data from the web is a lesson in the power of data science, because it exemplifies the computer-plus-human model of computing. It is also a nice entre into the process of building software, since custom scrapers are essentially software for scraping a specific website.

This workshop is for beginners, but it does assume that you have R installed, have a basic familiarity with how R works, and are familiar with at least one R interface (e.g. RStudio or the R command prompt). The workshop demonstrates multiple data formats, including lists and dataframes, and exploratory data visualization techniques, though this is not the focus of the workshop.
  
Tutorial is for a range of skill sets, still trying to avoid "something for everyone and everything for no one."

# General Idea of Webscraping
Since no two websites are the same, webscraping requires you to identify and exploit patterns in the code that renders websites. Each website is rendered by your browser from HTML, and the goal of webscraping is to parse the HTML that is sent to your browser into usable data. Generally speaking, the steps for webscraping are as follows: - access a web page from R, - tell R where to “look” on the page, - and manipulate the data in a usable format within R.

The first section will cover the magrittr package, which enables a piping operator to help us filter the through text. Without magrittr, we would have to assign a bunch of temporary variables or use nested functions to do operations on a string of HTML.

# A Prerequisite: `magrittr`
Rvest is designed to work with magrittr and the %>% piping notation. This can be a little goofy when you first see it, so let’s take a second to get oriented to the %>% piping notation and what it means. Piping is also useful for understanding other packages, such as ggplot and dplyr. Just remember the main advantage of piping is to minimize the need for nesting functions.

The magrittr package implements a piping function, which takes the output of an operation and “pipes” it to the next function.

## Why Scrape?
* Sometimes data are not downloadable.
* The only source might be a web page.
* You want to update the data (in the future).
* There are lots of 'high quality relational data' out there to be scraped.

### Other Presentations on Webscraping in R
* [http://cpsievert.github.io/slides/web-scraping/20150612/#8](http://cpsievert.github.io/slides/web-scraping/20150612/#8)
-* [https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/](https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/)

Generally speaking, the steps for webscraping are as follows:
- Inspect the page in your browser to determine if you really want to scrape it.
- Once you have decided, set aside some time and don't doubt yourself.
  - Access a web page from R;
  - Tell R where to "look" on the page;
  - and manipulate the data in a usable format within R.

## Now on to the scraping

1. Getting Familiar with the Website </br>

Because each website is different, we’ll be scraping a couple during this tutorial to give you an idea of the general principles (as well as a few examples of working code to start your next project). First up is SciStarter.org, a platform for advertising and joining citizen science projects. Take a look at the [projects page](https://scistarter.com/finder?phrase=&lat=&lng=&activity=At%20the%20beach&topic=&search_filters=&search_audience=&page=1#view-projects).


![Projects filtered by location "At the beach".](webscraping-imgs/scistarter.png)

To scrape the projects info from this page, we need to take a look at the HTML. 


You can do this by `right-clicking and selecting "view page source"` from Chrome, Firefox, Safari, and most other modern browsers. You should see something that looks like the image below.

![Source code for SciStarter.](webscraping-imgs/scistarter_html.png)


If you don't know HTML, this looks pretty daunting! Luckily, there are some built-in tools in your browser that will help you parse the code to find the relevant sections of the page. You can right-click and select the <b>"`Inspect Element`"</b> option. 


You can right-click and select the <b>"`Inspect Element`"</b> option. This allows you to interactively scroll over the page and see how each line of HTML corresponds to different parts of the web page. See the screen shot below for an example.

![Example of "Inspect Element" screen for SciStarter.](webscraping-imgs/scistarter_inspectelement.png)


Ok, now we're going to start trying to isolate the specific information we're after. Let's say our goal is to grab all the data from the first ten records on the page. We can see from the image above that the first piece of information we need is a link with the node "a" and the atttribute "href." Let's see if we can grab that using a basic function in the rvest package.

* First, let's install the `rvest` package with the command `install.packages('rvest')`. 
* Once it is installed, you can access the help by typing `?rvest` in your command prompt. 
* The documentation for specific functions can also be accessed the same way, such as `?html_node`.

The basic functions in `rvest` are powerful, and you should try to utilize the following functions when starting out a new project.

* `html_nodes()`:                 identifies HTML wrappers. 
  + `html_nodes(".class")`:       calls node based on css class
  + `html_nodes("#id")`:          calls node based on `<div>` id
  + `html_nodes(xpath="xpath")`:  calls node based on xpath (we'll cover this later)
* `html_attrs()`:                 identifies attributes (useful for debugging)
* `html_table()`:                 turns HTML tables into data frames
* `html_text()`:                  strips the HTML tags and extracts only the text

Note on plurals:
`html_node()` returns metadata for the , `html_nodes()` iterates over the matching nodes.


## html_nodes()
The `html_nodes()` function turns each HTML tag into a row in an R dataframe.

```{r, results='as-is'}
library(rvest)

# Define the url once.
URL <- "https://scistarter.com/finder?phrase=&lat=&lng=&activity=At%20the%20beach&topic=&search_filters=&search_audience=&page=1#view-projects"

scistarter_html <- read_html(URL)
scistarter_html

```

We're able to retrieve the same HTML code we saw in our browser. This is not useful yet, but it does show that we’re able to retrieve the same HTML code we saw in our browser. Now we will begin filtering through the HTML to find the data we’re after.

The data we want are stored in a table, which we can tell by looking at the "`Inspect Element`" window.

This grabs all the nodes that have links in them.
```{r}
scistarter_html %>%
  html_nodes("a") %>%
  head()
```
In a more complex example, we could use this to "crawl" the page, but that's for another day.

Every `div` on the page:
```{r}
scistarter_html %>%
  html_nodes("div") %>%
  head()
```

... the nav-tools `div`. This calls by css where `class=nav-tools`.
```{r}
scistarter_html %>%
  html_nodes("div.nav-tools") %>%
  head()
```

We can call the nodes by `id` as follows.
```{r}
scistarter_html %>%
  html_nodes("div#project-listing") %>%
  head()
```

All the tables as follows:
```{r}
scistarter_html %>%
  html_nodes("table") %>%
  head()
```
This will help us access one of `rvest`'s most powerful features, `html_table()`.

Now that it’s clear what html_node() does, lets look at the source code for the site again to see if we can combine rvest functions (and a some base R functions) to get at the right data.


## Putting it together
With the piping function and what we learned about `rvest`, we can now start scraping this page.

```{r}
scistarter_html %>%
  html_nodes("div#project-listing") %>%
  html_nodes("table") %>%
  html_table() %>%
  "["(1) %>% str()
```



Should we try using another function? Let's see.
```{r}

scistarter_html %>%
  html_nodes("div#project-listing") %>% #filter to the projec-listing div
  html_nodes("h3") # filter the tables in the project-listing div 

```

```{r}
scistarter_html %>%
  html_nodes("div#project-listing") %>% #filter to the projec-listing div
  html_nodes("h3") %>%                  # get the headings
  html_text() %>%                       #get the text, not the HTML tags
  gsub("^\\s+|\\s+$", "", .)            #strip the white space from the beginning and end of a string.

```
Now we have isolated the titles of each citizen science project. This will make up one of the columns of our dataframe, but it's not the whole thing.


Other columns are embedded in separate tables on the page. This is not ideal for webscraping, and there are multiple ways to access that information. 
- Example of a method of scraping the information from each table.
- Filter to tables
```{r}
scistarter_html %>%
  html_nodes("td") %>% #grab the <td> tags
  html_text() %>% # isolate the text from the html tages
  gsub("^\\s+|\\s+$", "", .) %>% #strip the white space from the beginning and end of a string.
  head(n=12) # take a peek at the first 12 records

```

## Putting it all together

```{r}
scistarter_html %>%
  html_nodes("td") %>% #grab the <td> tags
  html_text() %>% # isolate the text from the html tages
  gsub("^\\s+|\\s+$", "", .) %>% #strip the white space from the beginning and end of a string.
  head(n=12) # take a peek at the first 12 records

```

Notice the pattern here? 
Every third record is a goal, task, or location. We can use our base R knowledge to transform this into three columns that have only goals, tasks, or locations. After that, we will put the columns in a data.frame.


===
```{r}
page_list <- scistarter_html %>%
  html_nodes("td") %>%
  html_text() %>%
  gsub("^\\s+|\\s+$", "", .) #strip the white space from the beginning and end of a string.

goals <- page_list[seq(from=1, to=30,by=3)] # make a sequence to select the goals
task <- page_list[seq(from=2, to=30,by=3)]
location <- page_list[seq(from=3, to=30,by=3)]

title <- scistarter_html %>%
  html_nodes("div#project-listing") %>% #filter to the projec-listing div
  html_nodes("h3") %>%                  # get the headings
  html_text() %>%                       #get the text, not the HTML tags
  gsub("^\\s+|\\s+$", "", .) 

scistarter_df <- data.frame(title, goals, task, location)
```


Now you have scraped SciStarter's first project page! From here, you can write a loop that will build up a data frame from multiple pages by going to each page and scraping the data.


```{r, }
pages <- ceiling(832/10) # number of pages to go through
sci_df <- data.frame()

#for (page in (1:pages)) { Uncomment this if you want all the pages.
for (page in (1:5)) {

  
  print(paste0("geting data for page: " , page ))
  URL <- paste0("https://scistarter.com/finder?phrase=&lat=&lng=&activity=&topic=&search_filters=&search_audience=&page=", page, "#view-projects")
  
  sci_html <- read_html(URL)
  page_list <- sci_html %>%
    html_nodes("td") %>%
    html_text() %>%
    gsub("^\\s+|\\s+$", "", .) #strip the white space from the beginning and end of a string.

goal <- page_list[seq(from=1, to=30,by=3)]
task <- page_list[seq(from=2, to=30,by=3)]
location <- page_list[seq(from=3, to=30,by=3)]

title <- sci_html %>%
  html_nodes("div#project-listing") %>% #filter to the projec-listing div
  html_nodes("h3") %>% # get the headings
  html_text() %>% #get the text, not the HTML tags
  gsub("^\\s+|\\s+$", "", .) #strip the white space from the beginning and end of a string.

tmp <- data.frame(title, goal, task, location)
    if (pages == 1 ) {
        sci_df <- data.frame(tmp)
      } else {
        sci_df <- rbind(sci_df, tmp)  
      }
}

sci_df %>% str()


```


## Easier Targets - html_table()

Some websites publish their data in an easy-to-read table without offering the option to download the data. 

- Rvest is has a great tool built in for this, `html_table()`. 
- Using the functions listed above, isolate the table on the page. 
- Then pass the HTML table to `html_table()` and viola - a shiny R data frame is ready for you to analyze.

## html_table() example

Go to https://www.nis.gov.kh/cpi/Jan14.html and inspect the html.

```{r}
# To scrape a table from a website, the html_table() function can be a game-changer.
# But it doesn't give us the right output right away. 
URL2 <- "https://www.nis.gov.kh/cpi/Apr14.html"

# TIP: When debugging or building your scraper, assign a variable to the raw HTML.
# That way you only have to read it once
accounts <- read_html(URL2) 

table <- accounts %>%
  html_nodes("table") %>%
  html_table(header=T)

# You can clean up the table with the following code, or something like it. 
# table[[1]]
dict <- table[[1]][,1:2]
accounts_df <- table[[1]][6:18,-1]

names <- c('id', 'weight.pct', 'jan.2013', 'dec.2013', 'jan.2014', 'mo.pctch', 'yr.pctch', 'mo.cont', 'yr.cont')
colnames(accounts_df) <- names

accounts_df #%>% str()

```

## Using xpath
The final method for extracting data from a webpage is to call the data using it's xpath. Sometimes, we want very specific data from a website. Maybe we don't want only specific information from a table. 

The `xpath` option can be very useful for doing this, but it is not super intuitive. Think of this as directions you are giving `rvest` to the specific pice of data you're interested in scraping. 

For the last example, we'll scrape wunderground.com, inspired by a great tutorial on webscraping in Python from Nathan Yau's [Visualize This](http://book.flowingdata.com/).

```{r}
library(lubridate)
URL <- "https://www.wunderground.com/history/airport/WSSS/2016/1/1/DailyHistory.html?req_city=Singapore&req_statename=Singapore"

raw <- read_html(URL)
      
max <- raw %>% 
  html_nodes(xpath='//*[@id="historyTable"]/tbody/tr[3]/td[2]/span/span[1]')  %>%
  html_text() %>% as.numeric()
min <- raw %>%
  html_nodes(xpath='//*[@id="historyTable"]/tbody/tr[4]/td[2]/span/span[1]') %>%
  html_text() %>% as.numeric()
date <- ymd(paste("2016","1","1", sep="/"))

record <- data.frame(date, min, max)

record

```


What is going on here? 

* `//*[@id="historyTable"]/tbody/tr[3]/td[2]/span/span[1]`


* When you loaded `rvest`, you might have seen a friendly warning from R that `xml` package is a dependency. 
* `rvest` uses XML notation to read the HTML
* xpath is the "address" of the text in markup text.
* It isn't necessary to understand the deeper points regarding this (I certainly don't!), but it might demystify the next steps to learn about xpath. 


The address we supplied is `//*[@id="historyTable"]/tbody/tr[3]/td[2]/span/span[1]`. You can go back through the HTML using `view source` to reverse-engineer this xpath.

For a nice overview, check out [this StackOverflow response](http://stackoverflow.com/questions/3656414/explain-xpath-and-xquery-in-simple-terms).

Loop through the URLs like so:

```{r, results='hide'}
# go to https://www.wunderground.com/history/airport/WSSS/2017/3/5/DailyHistory.html?req_city=Singapore&req_statename=Singapore

years <- c(2016) # edit for the year(s) you want
months <- c(1:12)

for (y in years) {
  for (m in months) {
    if (m == c(4 || 6 || 9 || 11) ) {
      days <- c(1:31) # Apr, Jun, Sep, Nov have 31
    } else if (m == 2 && y %% 4 == 0 ) {
      days <- c(1:29) # leap year
    } else if (m == 2 && y %% 4 != 0 ) {
      days <- c(1:28) # non leap year Febs
    } else {
      days <- c(1:30) # All the rest have 30 days 
    }
    #for (d in days) {
    for (d in 1) {
      URL <- paste0("https://www.wunderground.com/history/airport/WSSS/", 
                    y, "/", 
                    m, "/",
                    d, "/DailyHistory.html?req_city=Singapore&req_statename=Singapore")
      print(URL) # try this to test before running the script
      
      raw <- read_html(URL)
      
      max <- raw %>% 
        html_nodes(xpath='//*[@id="historyTable"]/tbody/tr[3]/td[2]/span/span[1]')  %>%
        html_text() %>% as.numeric()
      min <- raw %>%
        html_nodes(xpath='//*[@id="historyTable"]/tbody/tr[4]/td[2]/span/span[1]') %>%
        html_text() %>% as.numeric()
      
      date <- ymd(paste(y,m,d, sep="/"))
      record <- data.frame(cbind(date, min, max))
      
      if ( date == "2016-01-01") {
        sing_temp <- record
      } else {
        sing_temp <- cbind(sing_temp, record)
      }
    }
  }
}
```

## Base R Scraper

```{r}
# Angel Hsu
# Scrape of RE100 members
# October 27, 2016

#setwd("~/Dropbox/NAZCA DATA 2016/RE100")
#URL of the HTML webpage 
co.names <- data.frame()
url <- "http://there100.org/companies"

x <- scan(url, what="", sep="\n")

# location of company names starts at line 2464
# <p><a href="http://www.there100.org/ikea" target="_blank"><img src="http://media.virbcdn.com/cdn_images/resize_1024x1365/71/87aded9e69e34239-ikea.jpg" /></a></p>
# <p>The IKEA Group is a home furnishing company with 336 stores in 28 countries. The company has committed to produce as much renewable energy as the total energy it consumes in its buildings by 2020. Alongside Swiss Re, IKEA Group is a founding partner of the RE100 campaign.</p>
start <- grep("The IKEA Group", x) # start of companies
end <- grep("S.p.A is an Italian", x) # end of companies 

# each company names is preceded by either a .jpg or a .png
sub <- x[start:end]
sel <- grep("jpg|png", sub)

co.names <- sub[sel+1]
# add back in bank of america
bofa <- grep("Bank of America", x)
co.names <- c(co.names, x[start], bofa)
co.names <- gsub("<.*?>", "", co.names)

write.csv(co.names, "RE100_2016.csv", row.names=F)

```

